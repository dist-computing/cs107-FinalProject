{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-a94b2e13-ad81-4057-826b-15f01ca813bc",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This software solves the issue of accurate differentiation. \n",
    "Accurate differentiation is important for many fields such as machine learning, numerical methods and optimization. \n",
    "Being able to accuratelly know the derivative of a non-linear function allows programers and mathematicians to quickly\n",
    "take derivatives when the focus of their research or project does not rely on the actual steps of differentiating a function,\n",
    "but simply finding the correct answer for the derivative of a given equation to move forward in their work. \n",
    "\n",
    "Unlike finite-difference numerical differentiation which is an approximation, automatic differentiation uses dual numbers to compute\n",
    "within machine precision exact derivatives based on elementary primitives, allowing for high-performance and highly accurate computation\n",
    "of numerical derivatives, which are useful for many fields.\n",
    "\n",
    "This software package will do just that for N number of variables and complex derivatives that would otherwise be \n",
    "extremely challenging to evaluate. This process should help minimize errors as compared to numerical methods.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-968f1c9a-43f2-40b4-a19c-5d413760cbee",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "# Background\n",
    "\n",
    "The main mathematical idea behind automatic differentiation is to break downs the process of differentiation \n",
    "into specific and iterable steps. We do so by breaking down each equation into to elementary arithmetic operations\n",
    "such as addition, subtraction, multiplication, division, power, expoential, logarithmic, cos, sin, tan, etc. \n",
    "To perform this process, automatic differentiation uses the power of the chain rule to brake down derivatives of composit functions into easily solvable components.\n",
    "The benefit of following this approach is that it allows the derivative evaluation to be as accurate as possible up to computer precision, unlike numerical differentiation. \n",
    "\n",
    "Automatic differentiation is benefitial because it can be used in two ways. The forward and the reverse accumulation. \n",
    "The workings of each of the two modes are described in more detail below.\n",
    "\n",
    "## Forward Accumulation \n",
    "In this mode we break down the equation by following chain rule as we would when doing it by hand. This approach is benefitial to compute accurate differentiation of pf matrix producs such as Jacobians. \n",
    "Because AD method inherently keeps track of all operations in a table, this becomes very efficient for evaulation other types higher order derivative based matrices such as Hessians. \n",
    "\n",
    "## Reverse Accumulation\n",
    "In this mode, the dependent variable is fixed and the derivative is computed backward recursively. This means that this accumulation type travels through the chainrule in a backward fashion, namely, from the outside toward the inside.\n",
    "Because of its similarity to backpropagation, namely, backpropagation of errors in multilayer perceptrons are a special case of reverse mode, this type of computational coding is a very efficient way of computing these backpropagations of error\n",
    "and ultimatly enables the ability to optimize a the weights in a neural network.\n",
    "\n",
    "### Example Evaluation Trace for a Simple Neural Network\n",
    "![](https://raw.githubusercontent.com/matheuscfernandes/cs107_matheus_fernandes/master/homework/HW4/P2_graph.png?token=ACDGXVLL3ZGJV5LQFRTYKXK723QAG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00002-e005a295-8715-45e3-ae90-83c8cbdbbbae",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-76967f3e-3039-4759-b65e-9c376f3d0af2",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "# How to use _AAD_ (\"Awesome Automatic Differentiation\")\n",
    "\n",
    "## Quick Initialization\n",
    "\n",
    "* `git clone` this package repository into your project directory.\n",
    "* Install dependencies using `pip`: `pip install -r requirements.txt`\n",
    "* Import our package using the following:\n",
    "    * `from AAD import as AD` *for AAD objects*\n",
    "    * `from AADFunction import as ADF` *for vector valued functions*\n",
    "    * `from solvers.Newton import AAD_Newton` *for Newton's Method*\n",
    "    * `from solvers.GradientDescent import AAD_grad` *for Gradient Descent*\n",
    "    * `from solvers.GMRES import AAD_GMRES` *for GMRES*\n",
    "* Run the tests; either using `pytest` or the manual test python script at `python ./tests/run_tests.py`.\n",
    "* Consult the documentation for quick examples to get started.\n",
    "\n",
    "## How to Install using Conda to create an environment\n",
    "It is good practice to use virtual environments (such as [Anaconda](https://github.com/Anaconda-Platform/anaconda-project)) to prevent tampering with your existing package installation in `python`.\n",
    "\n",
    "As a quick reference, you can install `AAD` with `conda` with a new environment using the following commands:\n",
    "```bash\n",
    "git clone git@github.com:dist-computing/cs107-FinalProject.git AAD\n",
    "cd AAD\n",
    "conda create --name AAD python=3.8\n",
    "conda install -n AAD requirements.txt\n",
    "```\n",
    "\n",
    "To activate the environment later, simply use `source activate AAD`.\n",
    "\n",
    "## Code example (univariate)\n",
    "You can create a driver script at the top level, e.g. `my_code.py`, and include the following code to use the `AAD` package:\n",
    "\n",
    "```python\n",
    "import AAD as AD\n",
    "import math\n",
    "#Evaluate the derivative of log(x) at x = 4\n",
    "x = 4 #initial value of x\n",
    "my_AD = AD.AADVariable(4) #creating the AAD object\n",
    "my_AD = AD.log(my_AD) #applying a function to the object to find both the value and derivative\n",
    "#Prints value of log and derivative at x = 4\n",
    "print(my_AD)\n",
    "```\n",
    "Answer:\n",
    "```\n",
    "AADVariable fun = 1.3862943611198906, der = 0.25\n",
    "```\n",
    "\n",
    "### Toy implementation of Newton's Method\n",
    "**You can retrieve the Jacobian (or scalar derivative) by tapping into the `.der` attribute of the `AAD`, or using the `.jacobian()` function on a `AADVariable` object.**\n",
    "\n",
    "With this you can solve for roots on functions, with the Newton's Method, i.e. for `sin(x)`:\n",
    "\n",
    "```python\n",
    "# Newton's Method for solving root (toy implementation, we have a better one!)\n",
    "# of f(x) = sin(x)\n",
    "x0 = AADVariable(2.0)\n",
    "for i in range(1, 20): # do 20 iterations maximum\n",
    "    fx = sin(x0)\n",
    "    x1 = x0.val - fx.val/fx.der\n",
    "    if abs(x0.val - x1) > 10e-6: # larger than minimum tolerance?\n",
    "        x0 = AADVariable(x1)\n",
    "    else:\n",
    "        break\n",
    "print(x0.val) # Final solution\n",
    "```\n",
    "This prints\n",
    "```\n",
    "3.1415926536808043\n",
    "```\n",
    "\n",
    "## Advanced Examples\n",
    "### Multivariate usage\n",
    "`AAD` fully supports vector-based inputs and outputs for its functions. For vector inputs, **variables are not named but instead identified by a positional vector** (\"seed vector\").\n",
    "We believe this implementation is more flexible and allows for arbitrary number of input components with a clear mathematical meaning.\n",
    "\n",
    "Multiple variables are communicated to the `AADVariable` dual number class using the second argument (derivative `der`) as a vector, i.e.\n",
    "`[1, 0]` is the first variable, `[0, 1]` is the second. Of course, specifying `(x+y)` as a composite variable using `[1, 1]` is also supported.\n",
    "\n",
    "A two-variable example:\n",
    "```python\n",
    "x = AADVariable(1.0, [1, 0]) # x = 1.0\n",
    "y = AADVariable(2.0, [0, 1]) # y = 2.0\n",
    "scalar = 6.0\n",
    "\n",
    "print(x + y - scalar) # AADVariable fun = -3.0, der = [1 1]\n",
    "print(x * y - scalar) # AADVariable fun = -4.0, der = [2. 1.]\n",
    "print((x + 0.5*y)/10) # 0.1x+0.05y, AADVariable fun = 0.2, der = [0.1  0.05]\n",
    "```\n",
    "\n",
    "As you can see, each indexed entry in the derivative correspond to the partial derivative with respect to that variable.\n",
    "\n",
    "**How can I add more variables?** The code is flexible and enables adding more variables without changing the previous seed vectors. e.g. the following will work:\n",
    "```python\n",
    "x = AADVariable(1.0, [1, 0]) # x = 1.0\n",
    "y = AADVariable(2.0, [0, 1]) # y = 2.0\n",
    "z = AADVariable(9.0, [0, 0, 1]) # z = 9.0\n",
    "print(x*y*z) # AADVariable fun = 18.0, der = [18.  9.  2.]\n",
    "```\n",
    "\n",
    "The `AADVariable` parser automatically zero-pads required derivative arrays, thus the following will work just fine:\n",
    "```python\n",
    "x = AADVariable(1.0, [1]) # x = 1.0\n",
    "y = AADVariable(2.0, [0, 1]) # y = 2.0\n",
    "z = AADVariable(9.0, [0, 0, 1]) # z = 9.0\n",
    "```\n",
    "\n",
    "For compatibility, **if only one variable is detected, all values are returned as scalars**, otherwise they return a `np.ndarray`.\n",
    "\n",
    "### Vector-valued functions\n",
    "For vector valued functions, they need to be wrapped in the `AADFunction` class to be tracked correctly. An example:\n",
    "\n",
    "```python\n",
    "x = AADVariable(1.0, [1]) # either not specifying der, [1], or [1 0] or [1,0,0] will all work, as above\n",
    "y = AADVariable(2.0, [0, 1]) \n",
    "f = AADFunction([x+y, x-y])\n",
    "print(f)\n",
    "```\n",
    "\n",
    "This prints:\n",
    "```\n",
    "[AADFunction fun = [3.0, -1.0], der = [[ 1  1]\n",
    " [ 1 -1]]]\n",
    " ```\n",
    "\n",
    "Where you can see that the Jacobian is `[1 1; 1 -1]`. The value and the derivative are retrieved from `f.val()` and `f.der()` methods, respectively.\n",
    "\n",
    "\n",
    "## How to use AAD's Optimization and Solver Suite\n",
    "AAD includes an awesome set of optimizers and solvers for scalar and vector-valued functions, including:\n",
    "* `Newton`'s method for finding roots. Usable for linear and non-linear scalar or vector-valued functions.\n",
    "* `GMRES` (generalized minimal residual method) for finding roots. Used for linear scalar or vector-valued functions.\n",
    "* A `GradientDescent` optimizer.\n",
    "\n",
    "### Examples\n",
    "#### Gradient Descent Optimizer\n",
    "To optimize using the Gradient Descent optimizer, specify the function using `def Function(X):` where X is a vector composed \n",
    "of the different variables. If the function only contains one variable X should be a vector with only one entry.  Note that the \n",
    "initial values also must be a vectored list with the same dimensions as the function.\n",
    "```python\n",
    "import AAD as AD\n",
    "from solvers.GradientDescent import AAD_grad\n",
    "x = AD.AADVariable(3, [1 ,0])\n",
    "y = AD.AADVariable(2, [0, 1])\n",
    "\n",
    "solve = AAD_grad.solve(lambda X: AD.abs(2*X[0]-12+2*X[1]**2),[x,y],0.001,progress_step=5000,max_iter=10000)\n",
    "```\n",
    "\n",
    "#### Newton's method root solver\n",
    "To solve roots using Newton's Method, simply specify the functions using Python's lambda function features.\n",
    "\n",
    "```python\n",
    "from solvers.Newton import AAD_Newton\n",
    "print(AAD_Newton.solve(lambda x, y, z, a: [x+4*y-z+a, a-2, y*5+z*0.1, x+2*a], [1, 1, 1, 1])) # [-4.0, 0.03703703703703704, -1.851851851851852, 2.0]\n",
    "print(AAD_Newton.solve(lambda x: [x*y+1, 3*x+5*y-1], [1])) # two sets of roots; the one solved here is [-1.1350416126511096, 0.8810249675906657]; depends on initial guess\n",
    "```\n",
    "\n",
    "What's happening under the hood?\n",
    "* The lambda function is translated to a set of `AADVariable` multi-variate inputs. **We use lambda functions so we can have \"faux\" symbolic math and it looks pretty.**\n",
    "* The function results are wrapped in `AADFunction` and automatically differentiated.\n",
    "* Newton's method is used to solve the equation.\n",
    "\n",
    "#### GMRES\n",
    "GMRES only works for linear functions. The method signature is similar to the Newton's method above:\n",
    "\n",
    "```python\n",
    "from solvers.GMRES import AAD_GMRES\n",
    "print(AAD_GMRES.solve(lambda x, y: [x+y+5, 3*x-y+1], [0.5, 1.5])) # [-1.5, -3.5]\n",
    "print(AAD_GMRES.solve(lambda x: [x-2], [300])) # [2.0]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00004-64c2302c-7514-4b33-a498-21af23a75ceb",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "# Organization\n",
    "\n",
    "## Directory structure and modules\n",
    "* We will have a main importable class that contains the directions for each function and how to use them. \n",
    "\n",
    "```\n",
    "README.md\n",
    "docs/                    Contains the documentation for the project.\n",
    "   README.md\n",
    "   documentation.ipynb   The main documentation.\n",
    "   milestone1.ipynb\n",
    "   milestone2.ipynb\n",
    "   milestone2_progress.ipynb\n",
    "   ...\n",
    "code/                     Source files\n",
    "   AADUtils.py              Utilities used as support functions (internal)\n",
    "   AAD.py                   Main constructor and dual number forward-mode implementation=\n",
    "   AADFunction.py           Vector-valued functions wrapper\n",
    "\n",
    "   solvers/\n",
    "      GradientDescent.py    Gradient-descent optimizer\n",
    "      Newton.py             Newton root finder (vector-valued, linear and nonlinear functions)\n",
    "      GMRES.py              Generalized Minimum-Residual Method root finder (vector-valued, linear functions)\n",
    "\n",
    "tests/                   Contains the test suite for the project\n",
    "   run_tests.py             Runs all of the tests it the folder\n",
    "   test_funtion.py         Tests Vector Valued Functions of AD Variables\n",
    "   test_gmres.py           Tests for GMRES\n",
    "   test_graddesc.py        Tests for gradient descent\n",
    "   test_main.py            Contains operations overrides and tests of complex functions for one variable\n",
    "   test_newton.py          Tests for newton's method\n",
    "   test_utils.py           Tests internal utilities\n",
    "```\n",
    "\n",
    "## Modules\n",
    "\n",
    "* The modules that we will use are `numpy`, `math`, `SimPy`, `SciPy`\n",
    "   * `numpy` will be used in order to evaluate and analyze arrays.\n",
    "   * `math` will be used in for its access to simple mathematical functions.\n",
    "   * `SimPy` will potentially be used to take symbolic derivatives and will be useful in our test suite. Additionally,\n",
    "   if a function is not in our elementary functions, we can use this module to help evaluate them.\n",
    "   * `SciPy` will be useful to test how our automatic differentiator compares to numeric derivatives (speed test).\n",
    "\n",
    "## Test suite\n",
    "* Our test suite will live inside the `tests` folder within our main repository.\n",
    "  * The tests cover all elementary functions, including overloaded operators (`__add__`, `__mul__`, etc.) and trigonometric functions (`sin`, `arccos`, ...) and other functions (`log`, `sqrt`)\n",
    "      * This also includes covering vector valued functions for similar examples\n",
    "  * Both the values and the derivatives generated by `AAD`'s forward mode are compared to analytical solutions.\n",
    "  * Our tests cover all three root solvers with single variable and multi-variable examples\n",
    "* The `run_tests.py` file runs all tests in our suite to make sure software works effectively for given coverage.\n",
    "* We will use `TravisCI` to test our suite. [View CI test results here](https://travis-ci.com/dist-computing/cs107-FinalProject).\n",
    "* Our Coverage on `Codecov` can be found here. We always maintain above 90% coverage. [View Coverage Here] (https://codecov.io/gh/dist-computing).\n",
    "* You can also manually run tests using `pytest` to ensure your installation and environment is correct.\n",
    "\n",
    "## Distribution and packaging\n",
    "* Currently `AAD` can be installed by cloning from the `git` repository using the command `git clone https://github.com/dist-computing/cs107-FinalProject.git`\n",
    "* In the future, we will distribute this package using `PyPI` and through the GitHub repository.\n",
    "* In the future, we will use `package`, which will package our package and we will not use a framework. We will not use a framework\n",
    "because this project is simple enough where we can manage without one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00005-08d08c82-371f-4d93-929b-134ffbf73010",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "# Implementation\n",
    "\n",
    "## Data structures\n",
    "The `AAD` class holds a scalar dual number and elementary functions required to perform algebra with dual numbers.\n",
    "\n",
    "## Classes and method signatures\n",
    "### The `AAD` dual number class\n",
    "This is a single variable dual number with forward-mode automatic differentiation. Includes representation and string functions, i.e. `__repr__` and `__str__`.\n",
    "\n",
    "Allows retrieval of the function value (i.e. `.val`) and derivative (scalar \"jacobian\") value (i.e. `.der`).\n",
    "\n",
    "### The `AAD` class\n",
    "Includes APIs used for performing Automatic Differentiation, as of the current version, AAD can handle single varibale differentiation that include terms using the following elementary functions:\n",
    "\n",
    "* `exp(x)` euler's number to the power of x\n",
    "* `log(x)` natural log of x\n",
    "* `sin(x)` sine of x\n",
    "* `sinh(x)` hyperbolic sin of x\n",
    "* `cos(x)` cosine of x \n",
    "* `cosh(x)` hyperbolic cosine of x\n",
    "* `tan(x)` tangent of x\n",
    "* `tanh(x)` hyperbolic tangent of x \n",
    "* `arcsin(x)` inverse sine of x\n",
    "* `arccos(x)` inverse cosine of x\n",
    "* `arctan(x)` inverse tangent of x\n",
    "* `sqrt(x)` square root of x\n",
    "\n",
    "Additionally, the Jacobian can be returned through calling its method:\n",
    "\n",
    "* `x.jacobian()` -- returns the jacobian of the AADVariable Object.\n",
    "\n",
    "### Attributes\n",
    "\n",
    "Our classes will have the following attributes\n",
    "* `__init__(val, der 0, name = none)` -- initialization and all variables where val is the value, der is the derivative and name is the name of the variable\n",
    "* `.der` -- provides the derivative of the function at the original point\n",
    "* `.val` -- provides the value of the function at the original point\n",
    "* `.name` -- used for future upgrades to software\n",
    "\n",
    "### External dependencies\n",
    "\n",
    "For matrix support, this software package requires `numpy`, `math`, `SimPy`, `SciPy`.\n",
    "\n",
    "### Vector Valued Functions\n",
    "\n",
    "For these functions we will evaluate each element of the vector valued function independently and then move on to the next element.\n",
    "We will store each evaluation in a numpy array - akin to a jacobian to output the correct derivative.\n",
    "\n",
    "### Elementary Operators - UPDATE THIS WITH ADDED FEATURES\n",
    "\n",
    "We will overload the addition, multiplication, subtraction, division, and power operators to work within our software using\n",
    "the baseline dunder methods and the reverse versions.\n",
    "\n",
    "The Following Elementary Operators have been overriden: \n",
    "* `__rmul__`,  `__mul__`, `__neg__`, `__add__`, `__radd__`, `__sub__`, `__rsub__`\n",
    "*  `__truediv__`, `__rtruediv__`, `__pow__`, `__rpow__`, `__repr__`\n",
    "\n",
    "These overrides accept real numbers, but do not assume real numbers. They can be used with multile AADVariable Objects\n",
    "\n",
    "Thse overriden operators allow for seamless operations and combindations of operations that include:\n",
    "* Power operations: `2**x` or `x**2`\n",
    "* Multiplication operations: `x*2` or `2*x` or `sin(x)*cos(x)`\n",
    "* Negative operations:  `x-3` or `3-x` or `sin(x)-cos(x)`\n",
    "* Division operations: `x/4` or `4/x` or `sin(x)/cos(x)`\n",
    "* Addition operations: `x+5` or `5+x` or `sin(x)+cos(x)`\n",
    "\n",
    "\n",
    "\n",
    "### Elementary functions\n",
    "Elementary functions are implemented using built-in Python `math` and the `numpy` package and include ready-made implementations for `Dual` numbers in this package.\n",
    "\n",
    "To deal with elementary functions such as `sin`, `log` and `exp` we will manually evaluate them and add them to a database to query continuously using the chain rule.\n",
    "* The derivative of `sin(x)` = `x' * cos (x)`\n",
    "* The derivative of `log(x)` = `x' * 1/x`\n",
    "* The derivative of `exp(x)` = `x' * exp(x)`\n",
    "\n",
    "Additionally, for other all other functions in the AAD class we perform the same process, with the chain rule.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00006-75d2c5cf-aabe-4316-9260-ee37a8dfde61",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "# New Feature\n",
    "### Newton's Method\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "### GMRES\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "$ f(x) = 3x-2$\n",
    "\n",
    "$ 0 = 3x-2$\n",
    "\n",
    "$ 0 = 3x-2$\n",
    "\n",
    "$ 2/3 = x$\n",
    "\n",
    "```python\n",
    "\n",
    "from solvers.Newton import AAD_Newton\n",
    "from solvers.GradientDescent import AAD_grad\n",
    "from solvers.GMRES import AAD_GMRES\n",
    "\n",
    "#Newton's Method\n",
    "x = AD.AADVariable(3, [1 ,0])\n",
    "initial_guess = [30]\n",
    "solve = AAD_Newton.solve(lambda x: [3*x-2], initial_guess)\n",
    "print(solve) #this prints [0.6666666666666666]\n",
    "\n",
    "#Gradient Descent\n",
    "x = AD.AADVariable(3, [1 ,0])\n",
    "gamma = .0001\n",
    "solve = AAD_grad.solve(lambda x: AD.abs(3*x[0]-2), [x], gamma)\n",
    "print(solve[0].val) #this prints [0.6666666666666666]\n",
    "\n",
    "#GMRES\n",
    "x = AD.AADVariable(3, [1 ,0])\n",
    "initial_guess = [30]\n",
    "solve = AAD_GMRES.solve(lambda x: [3*x-2], initial_guess)\n",
    "print(solve[0]) #this prints [0.6666666666666666]\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00007-176f4904-c814-4338-80a3-85d6f56f2a8e",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00007-b1097373-6d18-423e-a490-8915d1fce6e4",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "# Broader Impact & Inclusivity - BEN DO THIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00008-7b97f3c6-e799-4c41-a83a-270a5705534e",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "# Future Features & Ideas\n",
    "\n",
    "As an additional extension of this project, we would like to implement the reverse\n",
    "mode scheme of automaotic differentiation. Having this toolset in addition to optimizers \n",
    "is extremely valuable in the field of AI as it allows one to efficiently compute the backwardpropagation\n",
    "of errors in multilayer perceptions (i.e. neural networks). Being able\n",
    "to try different optimization schemes in addition to backward propogation will be useful in order to test the efficiency\n",
    "of different implementations. The implementation of backwardpropagation\n",
    "of neural networks is essnetiall a special case of reverse mode AD with an added set of \n",
    "elementary functions. These elementary functions are called activation functions some of\n",
    "which are already implemented in this version of the software. Here is a comprehensive list \n",
    "of new and already implemented functions we plan on incorporating into the future version\n",
    "of this software.\n",
    "\n",
    "\n",
    "**New Functions**: `relu`, `leaky relu`, `sigmoid`, `swish`, `softmax`, `binary step`\n",
    "\n",
    "**Already implemented**: `linear`, `tanh`\n",
    "\n",
    "Furthermore, as an additional step in this project, we would like to propose creating a framework\n",
    "for adding personalized optimizers and root finders so that users can define their own \n",
    "algorithm and test its performance with other algorithms for a particular reverse mode neural network.\n",
    "We would like to also create a vizualization toolset where users are able to compare the training, testing,\n",
    "and timing performce of their code in comparison to the built in optimizatio/root-finding algorithm.\n",
    "This will give the users an idea of what would be the best scheme for their particular application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00009-1bfa2ba1-560e-4b75-a727-f6bcfa3da0f6",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": []
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "79e9f0bc-45b3-48f9-a08d-1a7ed12a1dbb",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
