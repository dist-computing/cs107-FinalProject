{"cells":[{"cell_type":"markdown","source":"# Introduction\n\nThis software solves the issue of accurate differentiation. \nAccurate differentiation is important for many fields such as machine learning, numerical methods and optimization. \nBeing able to accuratelly know the derivative of a non-linear function allows programers and mathematicians to quickly\ntake derivatives when the focus of their research or project does not rely on the actual steps of differentiating a function,\nbut simply finding the correct answer for the derivative of a given equation to move forward in their work. \n\nUnlike finite-difference numerical differentiation which is an approximation, automatic differentiation uses dual numbers to compute\nwithin machine precision exact derivatives based on elementary primitives, allowing for high-performance and highly accurate computation\nof numerical derivatives, which are useful for many fields.\n\nThis software package will do just that for N number of variables and complex derivatives that would otherwise be \nextremely challenging to evaluate. This process should help minimize errors as compared to numerical methods.\n\n\n","metadata":{"tags":[],"cell_id":"00000-18ce91d6-8a38-4fc7-8f7d-a45caabd3981"}},{"cell_type":"markdown","source":"# Background\n\nThe main mathematical idea behind automatic differentiation is to break downs the process of differentiation \ninto specific and iterable steps. We do so by breaking down each equation into to elementary arithmetic operations\nsuch as addition, subtraction, multiplication, division, power, expoential, logarithmic, cos, sin, tan, etc. \nTo perform this process, automatic differentiation uses the power of the chain rule to brake down derivatives of composit functions into easily solvable components.\nThe benefit of following this approach is that it allows the derivative evaluation to be as accurate as possible up to computer precision, unlike numerical differentiation. \n\nAutomatic differentiation is benefitial because it can be used in two ways. The forward and the reverse accumulation. \nThe workings of each of the two modes are described in more detail below.\n\n## Forward Accumulation \nIn this mode we break down the equation by following chain rule as we would when doing it by hand. This approach is benefitial to compute accurate differentiation of pf matrix producs such as Jacobians. \nBecause AD method inherently keeps track of all operations in a table, this becomes very efficient for evaulation other types higher order derivative based matrices such as Hessians. \n\n## Reverse Accumulation\nIn this mode, the dependent variable is fixed and the derivative is computed backward recursively. This means that this accumulation type travels through the chainrule in a backward fashion, namely, from the outside toward the inside.\nBecause of its similarity to backpropagation, namely, backpropagation of errors in multilayer perceptrons are a special case of reverse mode, this type of computational coding is a very efficient way of computing these backpropagations of error\nand ultimatly enables the ability to optimize a the weights in a neural network.\n\n### Example Evaluation Trace for a Simple Neural Network\n![](https://raw.githubusercontent.com/matheuscfernandes/cs107_matheus_fernandes/master/homework/HW4/HW4-final/P2_graph.png?token=ACDGXVNZ5KUZRP2NTJI5UQC7WWAL6)\n","metadata":{"tags":[],"cell_id":"00001-d247f7f1-9f9d-4d2c-a225-bccdcbe43d76"}},{"cell_type":"markdown","source":"","metadata":{"tags":[],"cell_id":"00002-42114ae8-fd53-4a3d-beaa-6eb9a38e19ef"}},{"cell_type":"markdown","source":"# How to use _AAD_ (\"Awesome Automatic Differentiation\")\n\n## Initialization\n\n* `git clone` this package repository into your project directory.\n* Import our package using `import AAD`\n* Consult the documentation for quick examples to get started.\n\n## Code example\n```python\nimport AAD\nimport math\nmy_AD = AAD(math.log)\nmy_AD.derive()\nval, derivative = my_AD.evaluate(5) # for x = 5, type Dual\nprint(\"val = \", val, \", derivative = \", derivative)\n```\nAnswer:\n```\n<AAD: automatically differentiated in 10 iterations>\nval = 1.60943791, derivative = 0.200\n```","metadata":{"tags":[],"cell_id":"00003-96c7fad7-f07b-4059-8769-e0d78b18e205"}},{"cell_type":"markdown","source":"# Organization\n\n## Directory structure and modules\n* We will have a main importable class that contains the directions for each function and how to use them. \n\n```\nREADME.md\ndocs/                    Contains the documentation for the project.\n   README.md\n   milestone1.ipynb\n   milestone2.ipynb\n   milestone2_progress.ipynb\n   ...\ncode/                     Source files\n   types.py                 Shared types\n   shared.py                Shared utility functions\n   AAD.py                   Main constructor\n   forward.py               Forward mode code\n   reverse.py               Reverse mode code\ntests/                   Contains the test suite for the project\n```\n\n## Modules\n\n* The modules that we will use are `numpy`, `math`, `SimPy`, `SciPy`\n   * `numpy` will be used in order to evaluate and analyze arrays.\n   * `math` will be used in for its access to simple mathematical functions.\n   * `SimPy` will potentially be used to take symbolic derivatives and will be useful in our test suite. Additionally,\n   if a function is not in our elementary functions, we can use this module to help evaluate them.\n   * `SciPy` will be useful to test how our automatic differentiator compares to numeric derivatives (speed test).\n\n## Test suite\n* Our test suite will live inside the `tests` folder within our main repository.\n* We will use `TravisCI` to test our suite.\n\n## Distribution and packaging\n* We will distribute this package using `PyPI` and through the GitHub repository.\n* We will use `package`, which will package our package and we will not use a framework. We will not use a framework\nbecause this project is simple enough where we can manage without one.\n","metadata":{"tags":[],"cell_id":"00004-5d9f6e50-59f4-4e74-beb3-d3becc85e8c6"}},{"cell_type":"markdown","source":"# Implementation\n\n## Data structures\nThe core data structures of this software consist of a `Dual` number class holding dual numbers and auxiliary classes to hold elementary functions required to perform algebra with `Dual` numbers.\n\n## Classes and method signatures\n### The `Dual` number class\nIncludes representation and string functions, i.e. `__repr__` and `__str__`.\n\nAllows retrieval of the function value (i.e. `val()`) and derivative value (i.e. `deriv()`).\n\n### The `AAD` class\nIncludes APIs used for performing Automatic Differentiation, such as:\n* `derivative(f, x)`\n* `gradient(f, x)`\n* `jacobian(f, x)`\n* `hessian(f, x)`\n\n### Attributes\n\nOur classes will have the following attributes\n* `__init__(f)` -- initialization and all variables\n* `orderofoperations()`  -- finding the order of operations\n* `derive()` -- finding the derivatives of each function\n* `evaluate(X)` evaluating the functions and its derivatives at a given point\n\n### External dependencies\n\nFor matrix support, this software package requires `numpy`, `math`, `SimPy`, `SciPy`.\n\n### Vector Valued Functions\n\nFor these functions we will evaluate each element of the vector valued function independently and then move on to the next element.\nWe will store each evaluation in a numpy array - akin to a jacobian to output the correct derivative.\n\n### Elementary Operators\n\nWe will overload the addition, multiplication, subtraction, division, and power operators to work within our software using\nthe baseline dunder methods and the reverse versions (ex: `__rmul__` and `__mul__`)., and also the negation operator (`__neg__`).\n\n### Elementary functions\nElementary functions are implemented using built-in Python `math` and the `numpy` package and include ready-made implementations for `Dual` numbers in this package.\n\nTo deal with elementary functions such as `sin`, `log` and `exp` we will manually evaluate them and add them to a database to query.\n* The derivative of `sin(x)` = `x' * cos (x)`\n* The derivative of `log(x)` = `x' * 1/x`\n* The derivative of `exp(x)` = `x' * exp(x)`\n\nAdditionally, for other trigonometric functions, we will do them same.","metadata":{"tags":[],"cell_id":"00005-3b9eb820-78fd-4517-a44a-6442b38ba72c"}},{"cell_type":"markdown","source":"# Feedback\n## Milestone 1\n### 2/2 Introduction:\nWould have been nice to see more about why do we care about derivatives anyways and why is AAD a solution compared to other approaches? \n\n**Response:** *We have added a paragraph about finite-difference derivatives and AAD in the introduction. Additionally, we noted why automatic differentiation\nis preferred to numerical.*\n\n### 1/2 Background\nGood start to the background.  The flow could have been enhanced by presenting the evaluation trace and a computational graph.\n\nI would like to see more discussion on automatic differentiation. How do forward mode and reverse mode work?\n\nGoing forward, I would also like to see a discussion on what forward mode actually computes (Jacobian-vector product), the \"seed\" vector, and the efficiency of forward mode.\n\n**Response:** *We have added an example of the forward mode evaluation trace, the computational graph, and explanations for the modes*\n\n### 3/3 How to use\nGood job!\n\n**Response:** *We left this section as is because we got a perfect score and were only given positive feedback*\n\n### 3/3 Software Organization\nIt would be nice to include the directory structure tree.\n\n**Response:** *A proposed directory structure has been included.*\n\n### 4.5/5 Implementation\n1. How will you handle vector valued functions?\n2. Your implementation for elementary functions like `sin` and `log` is unclear.\n3. Will you implement operator overloading methods?\n\n**Response:** *We explained how we would implement elementary funtions manually and created sectsion for both elementary operators vector valued functions*\n\n13.5/15","metadata":{"tags":[],"cell_id":"00006-e8c24ab9-c96d-4676-a0f1-a390398a6305"}},{"cell_type":"markdown","source":"","metadata":{"tags":[],"cell_id":"00007-632f1277-09f8-464e-93b6-08d01a601068"}}],"nbformat":4,"nbformat_minor":2,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"},"deepnote_notebook_id":"e3733f52-2e3a-4fdb-8dde-c3ec615c0591","deepnote_execution_queue":[]}}