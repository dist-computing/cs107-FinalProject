{"cells":[{"cell_type":"markdown","source":"# Introduction\n\nThis software solves the issue of accurate differentiation. \nAccurate differentiation is important for many fields such as machine learning, numerical methods and optimization. \nBeing able to accuratelly know the derivative of a non-linear function allows programers and mathematicians to quickly\ntake derivatives when the focus of their research or project does not rely on the actual steps of differentiating a function,\nbut simply finding the correct answer for the derivative of a given equation to move forward in their work. \n\nUnlike finite-difference numerical differentiation which is an approximation, automatic differentiation uses dual numbers to compute\nwithin machine precision exact derivatives based on elementary primitives, allowing for high-performance and highly accurate computation\nof numerical derivatives, which are useful for many fields.\n\nThis software package will do just that for N number of variables and complex derivatives that would otherwise be \nextremely challenging to evaluate. This process should help minimize errors as compared to numerical methods.\n\n\n","metadata":{"tags":[],"cell_id":"00000-18ce91d6-8a38-4fc7-8f7d-a45caabd3981","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"# Background\n\nThe main mathematical idea behind automatic differentiation is to break downs the process of differentiation \ninto specific and iterable steps. We do so by breaking down each equation into to elementary arithmetic operations\nsuch as addition, subtraction, multiplication, division, power, expoential, logarithmic, cos, sin, tan, etc. \nTo perform this process, automatic differentiation uses the power of the chain rule to brake down derivatives of composit functions into easily solvable components.\nThe benefit of following this approach is that it allows the derivative evaluation to be as accurate as possible up to computer precision, unlike numerical differentiation. \n\nAutomatic differentiation is benefitial because it can be used in two ways. The forward and the reverse accumulation. \nThe workings of each of the two modes are described in more detail below.\n\n## Forward Accumulation \nIn this mode we break down the equation by following chain rule as we would when doing it by hand. This approach is benefitial to compute accurate differentiation of pf matrix producs such as Jacobians. \nBecause AD method inherently keeps track of all operations in a table, this becomes very efficient for evaulation other types higher order derivative based matrices such as Hessians. \n\n## Reverse Accumulation\nIn this mode, the dependent variable is fixed and the derivative is computed backward recursively. This means that this accumulation type travels through the chainrule in a backward fashion, namely, from the outside toward the inside.\nBecause of its similarity to backpropagation, namely, backpropagation of errors in multilayer perceptrons are a special case of reverse mode, this type of computational coding is a very efficient way of computing these backpropagations of error\nand ultimatly enables the ability to optimize a the weights in a neural network.\n\n### Example Evaluation Trace for a Simple Neural Network\n![](https://raw.githubusercontent.com/matheuscfernandes/cs107_matheus_fernandes/master/homework/HW4/HW4-final/P2_graph.png?token=ACDGXVNZ5KUZRP2NTJI5UQC7WWAL6)\n","metadata":{"tags":[],"cell_id":"00001-d247f7f1-9f9d-4d2c-a225-bccdcbe43d76","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"tags":[],"cell_id":"00002-42114ae8-fd53-4a3d-beaa-6eb9a38e19ef","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"# How to use _AAD_ (\"Awesome Automatic Differentiation\")\n\n## Quick Initialization\n\n* `git clone` this package repository into your project directory.\n* Install dependencies using `pip`: `pip install -r requirements.txt`\n* Import our package using `import AAD`\n* Run the tests; either using `pytest` or the manual test python script at `python ./tests/test_1.py`.\n* Consult the documentation for quick examples to get started.\n\n## How to Install using Conda to create an environment\nIt is good practice to use virtual environments (such as [Anaconda](https://github.com/Anaconda-Platform/anaconda-project)) to prevent tampering with your existing package installation in `python`.\n\nAs a quick reference, you can install `AAD` with `conda` with a new environment using the following commands:\n```bash\ngit clone git@github.com:dist-computing/cs107-FinalProject.git AAD\ncd AAD\nconda create --name AAD python=3.8\nconda install -n AAD requirements.txt\n```\n\nTo activate the environment later, simply use `source activate AAD`.\n\n## Code example\nYou can create a driver script at the top level, e.g. `my_code.py`, and include the following code to use the `AAD` package:\n\n```python\nimport AAD as ad\nimport math\n#Evaluate the derivative of log(x) at x = 4\nx = 4 #initial value of x\nmy_AD = ad.AADVariable(4) #creating the AAD object\nmy_AD = ad.log(my_AD) #applying a function to the object to find both the value and derivative\n#Prints value of log and derivative at x = 4\nprint(my_AD)\n```\nAnswer:\n```\nAADVariable fun = 1.3862943611198906, der = 0.25\n```\n\n### Newton's Method for solving roots\n**You can retrieve the Jacobian (or scalar derivative) by tapping into the `.der` attribute of the `AAD`, or using the `.jacobian()` function on a `AADVariable` object.**\n\nWith this you can solve for roots on functions, with the Newton's Method, i.e. for `sin(x)`:\n\n```python\n# Newton's Method for solving root\n# of f(x) = sin(x)\nx0 = AADVariable(2.0)\nfor i in range(1, 20): # do 20 iterations maximum\n    fx = sin(x0)\n    x1 = x0.val - fx.val/fx.der\n    if abs(x0.val - x1) > 10e-6: # larger than minimum tolerance?\n        x0 = AADVariable(x1)\n    else:\n        break\nprint(x0.val) # Final solution\n```\nThis prints\n```\n3.1415926536808043\n```","metadata":{"tags":[],"cell_id":"00003-96c7fad7-f07b-4059-8769-e0d78b18e205","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"# Organization\n\n## Directory structure and modules\n* We will have a main importable class that contains the directions for each function and how to use them. \n\n```\nREADME.md\ndocs/                    Contains the documentation for the project.\n   README.md\n   milestone1.ipynb\n   milestone2.ipynb\n   milestone2_progress.ipynb\n   ...\ncode/                     Source files\n   types.py                 Shared types (future use)\n   shared.py                Shared utility functions (future use)\n   AAD.py                   Main constructor\ntests/                   Contains the test suite for the project\n   test_1.py               Contains operations overrides and tests of complex functions for one variable\n```\n\n## Modules\n\n* The modules that we will use are `numpy`, `math`, `SimPy`, `SciPy`\n   * `numpy` will be used in order to evaluate and analyze arrays.\n   * `math` will be used in for its access to simple mathematical functions.\n   * `SimPy` will potentially be used to take symbolic derivatives and will be useful in our test suite. Additionally,\n   if a function is not in our elementary functions, we can use this module to help evaluate them.\n   * `SciPy` will be useful to test how our automatic differentiator compares to numeric derivatives (speed test).\n\n## Test suite\n* Our test suite will live inside the `tests` folder within our main repository.\n  * The tests cover all elementary functions, including overloaded operators (`__add__`, `__mul__`, etc.) and trigonometric functions (`sin`, `arccos`, ...) and other functions (`log`, `sqrt`)\n  * Both the values and the derivatives generated by `AAD`'s forward mode are compared to analytical solutions.\n* We will use `TravisCI` to test our suite. [View CI test results here](https://travis-ci.com/dist-computing/cs107-FinalProject).\n* Our Coverage on `Codecov` can be found here. We always maintain above 90% coverage. [View Coverage Here] (https://codecov.io/gh/dist-computing).\n* You can also manually run tests using `pytest` to ensure your installation and environment is correct.\n\n## Distribution and packaging\n* Currently `AAD` can be installed by cloning from the `git` repository using the command `git clone https://github.com/dist-computing/cs107-FinalProject.git`\n* In the future, we will distribute this package using `PyPI` and through the GitHub repository.\n* In the future, we will use `package`, which will package our package and we will not use a framework. We will not use a framework\nbecause this project is simple enough where we can manage without one.\n","metadata":{"tags":[],"cell_id":"00004-5d9f6e50-59f4-4e74-beb3-d3becc85e8c6","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"# Implementation\n\n## Data structures\nThe `AAD` class holds a scalar dual number and elementary functions required to perform algebra with dual numbers.\n\n## Classes and method signatures\n### The `AAD` dual number class\nThis is a single variable dual number with forward-mode automatic differentiation. Includes representation and string functions, i.e. `__repr__` and `__str__`.\n\nAllows retrieval of the function value (i.e. `.val`) and derivative (scalar \"jacobian\") value (i.e. `.der`).\n\n### The `AAD` class\nIncludes APIs used for performing Automatic Differentiation, as of the current version, AAD can handle single varibale differentiation that include terms using the following elementary functions:\n\n* `exp(x)` euler's number to the power of x\n* `log(x)` natural log of x\n* `sin(x)` sine of x\n* `sinh(x)` hyperbolic sin of x\n* `cos(x)` cosine of x \n* `cosh(x)` hyperbolic cosine of x\n* `tan(x)` tangent of x\n* `tanh(x)` hyperbolic tangent of x \n* `arcsin(x)` inverse sine of x\n* `arccos(x)` inverse cosine of x\n* `arctan(x)` inverse tangent of x\n* `sqrt(x)` square root of x\n\nAdditionally, the Jacobian can be returned through calling its method:\n\n* `x.jacobian()` -- returns the jacobian of the AADVariable Object.\n\n### Attributes\n\nOur classes will have the following attributes\n* `__init__(val, der 0, name = none)` -- initialization and all variables where val is the value, der is the derivative and name is the name of the variable\n* `.der` -- provides the derivative of the function at the original point\n* `.val` -- provides the value of the function at the original point\n* `.name` -- used for future upgrades to software\n\n### External dependencies\n\nFor matrix support, this software package requires `numpy`, `math`, `SimPy`, `SciPy`.\n\n### Vector Valued Functions\n\nFor these functions we will evaluate each element of the vector valued function independently and then move on to the next element.\nWe will store each evaluation in a numpy array - akin to a jacobian to output the correct derivative.\n\n### Elementary Operators\n\nWe will overload the addition, multiplication, subtraction, division, and power operators to work within our software using\nthe baseline dunder methods and the reverse versions.\n\nThe Following Elementary Operators have been overriden: \n* `__rmul__`,  `__mul__`, `__neg__`, `__add__`, `__radd__`, `__sub__`, `__rsub__`\n*  `__truediv__`, `__rtruediv__`, `__pow__`, `__rpow__`, `__repr__`\n\nThese overrides accept real numbers, but do not assume real numbers. They can be used with multile AADVariable Objects\n\nThse overriden operators allow for seamless operations and combindations of operations that include:\n* Power operations: `2**x` or `x**2`\n* Multiplication operations: `x*2` or `2*x` or `sin(x)*cos(x)`\n* Negative operations:  `x-3` or `3-x` or `sin(x)-cos(x)`\n* Division operations: `x/4` or `4/x` or `sin(x)/cos(x)`\n* Addition operations: `x+5` or `5+x` or `sin(x)+cos(x)`\n\n\n\n### Elementary functions\nElementary functions are implemented using built-in Python `math` and the `numpy` package and include ready-made implementations for `Dual` numbers in this package.\n\nTo deal with elementary functions such as `sin`, `log` and `exp` we will manually evaluate them and add them to a database to query continuously using the chain rule.\n* The derivative of `sin(x)` = `x' * cos (x)`\n* The derivative of `log(x)` = `x' * 1/x`\n* The derivative of `exp(x)` = `x' * exp(x)`\n\nAdditionally, for other all other functions in the AAD class we perform the same process, with the chain rule.\n    ","metadata":{"tags":[],"cell_id":"00005-3b9eb820-78fd-4517-a44a-6442b38ba72c","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"# Future Features/Implementation\n\nTo extend this project further, we will consider the implementation of the \nreverse mode scheme of automatic differentiation. Having this toolset is extremely\nvaluable in the field of AI as it allows one to efficiently compute the backwardpropagation\nof errors in multilayer perceptions (i.e. neural networks). The implementation of backwardpropagation\nof neural networks is essnetiall a special case of reverse mode AD with an added set of \nelementary functions. These elementary functions are called activation functions some of\nwhich are already implemented in this version of the software. Here is a comprehensive list \nof new and already implemented functions we plan on incorporating into the future version\nof this software.\n\n\n**New Functions**: `relu`, `leaky relu`, `sigmoid`, `swish`, `softmax`, `binary step`\n\n**Already implemented**: `linear`, `tanh`\n\n### Structural Changes\nWe will move the forward mode implementation to a new `forward.py` and create the backpropagation implementation\nin `reverse.py`. Shared functions, classes and types across both implementations can be refactored into a `shared.py`\nshared class.\n\n### Feature Additions\n\nWe will enhance the capability of the `.jacobian()` method. Currently,\nthis is just the first derivative of tha single variable. However, as more variables are implemented\nwe will allow access to the Jacobian so that we can use it as a matrix with multivariable features.","metadata":{"tags":[],"cell_id":"00006-3b153b1a-20ea-411a-a8e5-25d38219b87c","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"# Feedback\n## Milestone 1\n### 2/2 Introduction:\nWould have been nice to see more about why do we care about derivatives anyways and why is AAD a solution compared to other approaches? \n\n**Response:** *We have added a paragraph about finite-difference derivatives and AAD in the introduction. Additionally, we noted why automatic differentiation\nis preferred to numerical.*\n\n### 1/2 Background\nGood start to the background.  The flow could have been enhanced by presenting the evaluation trace and a computational graph.\n\nI would like to see more discussion on automatic differentiation. How do forward mode and reverse mode work?\n\nGoing forward, I would also like to see a discussion on what forward mode actually computes (Jacobian-vector product), the \"seed\" vector, and the efficiency of forward mode.\n\n**Response:** *We have added an example of the forward mode evaluation trace, the computational graph, and explanations for the modes*\n\n### 3/3 How to use\nGood job!\n\n**Response:** *We left this section as is because we got a perfect score and were only given positive feedback*\n\n### 3/3 Software Organization\nIt would be nice to include the directory structure tree.\n\n**Response:** *A proposed directory structure has been included.*\n\n### 4.5/5 Implementation\n1. How will you handle vector valued functions?\n2. Your implementation for elementary functions like `sin` and `log` is unclear.\n3. Will you implement operator overloading methods?\n\n**Response:** *We explained how we would implement elementary funtions manually and created sectsion for both elementary operators vector valued functions*\n\n13.5/15","metadata":{"tags":[],"cell_id":"00006-e8c24ab9-c96d-4676-a0f1-a390398a6305","deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":2,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"},"deepnote_notebook_id":"e3733f52-2e3a-4fdb-8dde-c3ec615c0591","deepnote_execution_queue":[]}}