{"cells":[{"cell_type":"markdown","source":"# Final Video Presentation: CS 207\n\n## Awesome Automatic Differentiation (AAD) & Root Finder\n\n### Haipeng Lin, Matheus Fernandes, & Benjamin Manning","metadata":{"tags":[],"cell_id":"00001-a7ef32ba-475a-48f4-a44f-636dfbfd85de","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### Presentation Summary\n\n - Background & Introduction\n \n - Implementation\n \n - Software Organization\n \n - How to Use\n \n - Additional Features/Extension\n\n - Impact and Inclusivity\n \n - Future Work & Other Possible Extensions","metadata":{"tags":[],"cell_id":"00001-cecfe5bc-f1d3-4847-8de9-a6899bb2c5c8","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Background & Introduction to Awesome Automatic Differentiation \n\n#### What is Automatic Differentation?\n- A fast and easy derivative method that avoids the use of hyperparamters and iteration.\n- Uses dual numbers to compute within machine precision exact derivatives based on elementary primitives\n\n#### Why is it Useful?\n- Accurate differentiation is important for many fields such as machine learning, numerical methods and optimization. \n- Allows programers and mathematicians to quickly take derivatives when the focus of their research or project does not rely on the actual steps of differentiating a function.\n\n#### What is 'The Math'?\n- The Forward Mode\n- Break down the process of differentiation into specific and repeatable steps\n- Assess each part of an equation as a part of an elementary operation or fucntion\n- Use chain rule to brake down derivatives of composit functions into easily solvable components\n\n#### Forward Mode Simple Example\n\n**Orginal Function**\n\n$ f(x,y) = 3xy$ *at* $(x,y) = (2,3)$\n\n$ f(2,3) = 3 \\times 2 \\times 3 = 18$\n\n**Solve Taking Partial Derivatives**\n\n$ \\frac{\\partial f}{\\partial x} (x,y) = 3y $\n\n$ \\frac{\\partial f}{\\partial y} (x,y) = 3x $\n\n$ \\frac{\\partial f}{\\partial x} (2,3) = 9 $\n\n$ \\frac{\\partial f}{\\partial y} (2,3) = 6 $\n\n\n|   Trace  |   Elem Function | Current Val | Func Der | $\\Delta_x$ | $\\Delta_y$ |\n| :---: | :-----------: |:----------: | :------: | :--------: | :--------: |\n| $ v_1 $  |    x    |2 | $x'$| 1 | 0| \n| $ v_2 $  |    y    |3 | $y'$ | 0 | 1 | \n| $ v_3 $  |   $3v_1$   |6 | $3v_1'$ | 3 | 0 | \n| $ f(x,y) = v_4 $ | $v_3 \\times v_2$ | $6 \\times 3 = 18 $ |$v_3' \\times v_2 + v_3 \\times v_2'$ | $3 \\times 3 + 6 \\times 0 = 9 $|$0 \\times 3 + 6 \\times 1 = 6$ |\n\n**Current Value is the same as The original Function**\n\n$f(2,3) = 3 \\times 2 \\times 3 = 18 = v_4 $\n\n**Same Answer as Partial Derivatives**\n\n$\\Delta_x = 9 = \\frac{\\partial f}{\\partial x} (2,3)$\n\n$\\Delta_y = 6 = \\frac{\\partial f}{\\partial y} (2,3)$\n\n","metadata":{"tags":[],"cell_id":"00002-6b69e919-7e5b-4f00-9bb4-a33920eeeca5","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### Example Trace Evaluation of Above Problem\n\n$ v_1 = x = X_1$\n\n$ v_2 = y = X_0$\n\n$ v_3 = 3v_1 = V_1$\n\n$ v_4 = f(x,y) = v_2 \\times v_3 =  f$\n\n\n![](trace_eval.png)\n","metadata":{"tags":[],"cell_id":"00003-0e5f3d4e-87f8-4de4-b7d6-623342b7e7ca","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Software Organization\n\n\n```\nREADME.md\ndemos/                   Final Presentation with appropriate helper files\n   Presentation.ipynb    Final Presentatoin\n   ...\ndocs/                    Contains the documentation for the project.\n   README.md\n   documentation.ipynb   The main documentation.\n   milestone1.ipynb\n   milestone2.ipynb\n   milestone2_progress.ipynb\n   ...\n\ncode/                     Source files\n   AADUtils.py              Utilities used as support functions (internal)\n   AAD.py                   Main constructor and dual number forward-mode implementation=\n   AADFunction.py           Vector-valued functions wrapper\n   ...\n   solvers/\n      GradientDescent.py    Gradient-descent optimizer\n      Newton.py             Newton root finder (vector-valued, linear and nonlinear functions)\n      GMRES.py              Generalized Minimum-Residual Method root finder (vector-valued, linear functions)\n\ntests/                   Contains the test suite for the project\n   run_tests.py             Runs all of the tests it the folder\n   test_funtion.py         Tests Vector Valued Functions of AD Variables\n   test_gmres.py           Tests for GMRES\n   test_graddesc.py        Tests for gradient descent\n   test_main.py            Contains operations overrides and tests of complex functions for one variable\n   test_newton.py          Tests for newton's method\n   test_utils.py           Tests internal utilities\n```\n\n\n","metadata":{"tags":[],"cell_id":"00005-1b54d00a-eb94-44b6-b994-7705b8f0646c","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Implementation\n\n### Elementary Functions:\n\n* `exp(x)` euler's number to the power of x\n* `log(x)` natural log of x\n* `sin(x)` sine of x\n* `sinh(x)` hyperbolic sin of x\n* `cos(x)` cosine of x \n* `cosh(x)` hyperbolic cosine of x\n* `tan(x)` tangent of x\n* `tanh(x)` hyperbolic tangent of x \n* `arcsin(x)` inverse sine of x\n* `arccos(x)` inverse cosine of x\n* `arctan(x)` inverse tangent of x\n* `sqrt(x)` square root of x\n\n### Operation Overrides\n\n* `__rmul__`,  `__mul__`, `__neg__`, `__add__`, `__radd__`, `__sub__`, `__rsub__`\n*  `__truediv__`, `__rtruediv__`, `__pow__`, `__rpow__`, `__repr__`\n\nThese overrides accept real numbers, but do not assume real numbers. They can be used with multile AADVariable Objects\n\nThse overriden operators allow for seamless operations and combindations of operations that include:\n* Power operations: `2**x` or `x**2`\n* Multiplication operations: `x*2` or `2*x` or `sin(x)*cos(x)`\n* Negative operations:  `x-3` or `3-x` or `sin(x)-cos(x)`\n* Division operations: `x/4` or `4/x` or `sin(x)/cos(x)`\n* Addition operations: `x+5` or `5+x` or `sin(x)+cos(x)`\n","metadata":{"tags":[],"cell_id":"00002-e658a780-beab-4e69-a697-46ae58b2498b","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## How to Use","metadata":{"tags":[],"cell_id":"00002-a473d482-0271-4498-8298-ef9cf414d715","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Quick Initialization\n\n* `git clone` this package repository into your project directory.\n* Install dependencies using `pip`: `pip install -r requirements.txt`\n* Import our package using the following:\n    * `from AAD import as AD` *for AAD objects*\n    * `from AADFunction import as ADF` *for vector valued functions*\n    * `from solvers.Newton import AAD_Newton` *for Newton's Method*\n    * `from solvers.GradientDescent import AAD_grad` *for Gradient Descent*\n    * `from solvers.GMRES import AAD_GMRES` *for GMRES*\n\nAs a quick reference, you can install `AAD` with `conda` with a new environment using the following commands:\n```bash\ngit clone git@github.com:dist-computing/cs107-FinalProject.git AAD\ncd AAD\nconda create --name AAD python=3.8\nconda install -n AAD requirements.txt\n```\n\nTo activate the environment later, simply use `source activate AAD`.","metadata":{"tags":[],"cell_id":"00007-9a220420-0bec-4a5b-9572-f972f72645ea","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Github Pages Website Publishing\n\nTo efficiently publish the github pages website we further streamlined the work with bash scripting. In the script\nincluded in the package a person can keep two folders with two branches, one for github pages and one for the gh-pages. \nBy running the bash script below, you can update the website based on the documentation file. `./UpdateIndexFromDocomentation.sh`\n\n```bash\n#!/bin/bash\n\ncd ../website/\n\n#copy files from main repo\ncp ../cs107-FinalProject/docs/documentation.ipynb ./documentation.ipynb\n\n#convert notebook to markdown\njupyter nbconvert $(pwd)/documentation.ipynb --to markdown --output $(pwd)/index.md\n\n#add title to file\nsed -i '1s/^/---\\n/' index.md\nsed -i '1s/^/ title: About the code \\n/' index.md\nsed -i '1s/^/---\\n/' index.md\n\n#commit, push and publish\ngit add index.md documentation.ipynb\ngit commit -m 'updated index.md with documentation <automated>'\ngit push\n\n```\n","metadata":{"tags":[],"cell_id":"00008-f2ec0cc7-9f4a-4b37-9535-5a8a04726de5","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### Requirements & Dependencies","metadata":{"tags":[],"cell_id":"00007-5f77fa82-ba94-4600-b3bf-38cf8786df83","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00002-2a0d2d1e-65db-48ee-8e32-7196162e47ba","output_cleared":false,"source_hash":"a8241dee","execution_millis":0,"execution_start":1607543394475,"deepnote_cell_type":"code"},"source":"import sys\nsys.path.insert(1, '../AAD/')\n#importing Dependencies\nimport numpy as np\nimport AAD as AD\nimport AADFunction as ADF\nimport math\n","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### One Dimensional Example - simple case","metadata":{"tags":[],"cell_id":"00003-73fc9cf4-d9fb-4a60-9659-8e26c75f3ab5","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"**Function & Evaluation**\n\n$ f(x) = \\ln(x) $ *at x = 4*\n\n$ f(x) = \\ln(4) = 1.3862943611198906 $\n\n**Derivative & Evaluation**\n\n$ f'(x) = \\frac{{1}}{x} $\n\n$ f'(4) = \\frac{{1}}{4} = 0.25 $\n","metadata":{"tags":[],"cell_id":"00004-c4ea52c5-9f68-4af9-bce9-40f65ea69ac3","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00003-eea5e40c-2a33-41ba-981a-9135029338a6","output_cleared":false,"source_hash":"9897f63a","execution_millis":2,"execution_start":1607543394940,"deepnote_cell_type":"code"},"source":"x = 4 #initial value of x\n\nmy_AD = AD.AADVariable(x) #creating the AAD object\n\nmy_AD = AD.log(my_AD) #applying a function to the object to find both the value and derivative\n\n#Prints value of log and derivative at x = 4\nprint(my_AD)","execution_count":null,"outputs":[{"name":"stdout","text":"AADVariable fun = 1.3862943611198906, der = 0.25\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### One Dimensional Example - Complex Case\n\n**Function & Evaluation**\n\n$ f(x) = e^{tan(x)}$ *at x = $\\pi$*\n\n$ f(\\pi) = e^{tan(\\pi)} = 1 $\n\n**Derivative & Evaluation**\n\n$ f'(x) = \\frac{{1}}{cos^2(x)} \\times e^{tan(x)} = \\frac{{e^{tan(x)}}}{cos^2(x)}$\n\n$ f'(\\pi) = \\frac{{e^{tan(\\pi)}}}{cos^2(\\pi)} = 1$","metadata":{"tags":[],"cell_id":"00006-f4420bcc-922b-4444-8c75-1453dcc69b54","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00004-6b987c15-fa87-4575-ae5b-c6f9e658a97d","output_cleared":false,"source_hash":"e831c403","execution_millis":2,"execution_start":1607543395543,"deepnote_cell_type":"code"},"source":"x = math.pi #initial value of x\n\nmy_AD = AD.AADVariable(x) #creating the AAD object\n\nmy_AD = AD.exp(AD.tan(my_AD)) #applying a function to the object to find both the value and derivative\n\n#Prints value of e^tan(x) and derivative at x = pi\nprint(my_AD)","execution_count":null,"outputs":[{"name":"stdout","text":"AADVariable fun = 0.9999999999999999, der = 0.9999999999999999\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Multi Dimensional Example - simple case","metadata":{"tags":[],"cell_id":"00015-7afd1c18-142d-49e7-8b8d-3064541c4975","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"**Function & Evaluation**\n\n$ f(x,y) = x + y - 6 $ *at x = 1.0 and y = 2.0*\n\n$ f(1,2) = 1.0 + 2.0 - 6 = - 3.0$\n\n**Partial Derivative with respect to X**\n\n$ \\frac{\\partial f}{\\partial x} (x,y) = 1.0 $\n\n$ \\frac{\\partial f}{\\partial x} (1.0,2.0) = 1.0 $\n\n**Partial Derivative with respect to Y**\n\n$ \\frac{\\partial f}{\\partial y} (x,y) = 1.0 $\n\n$ \\frac{\\partial f}{\\partial y} (1.0,2.0) = 1.0 $\n\n**Derivative Evaluation**\n\n$ f'(1.0,2.0) = \\begin{bmatrix}\n1.0 & 1.0 \n\\end{bmatrix}$ \n\n\n","metadata":{"tags":[],"cell_id":"00016-c238fcf8-b71b-4124-a64f-00cfd8e5582d","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00007-657e4714-261b-4c22-837c-d780ecca86ee","output_cleared":false,"source_hash":"9339f22c","execution_millis":2,"execution_start":1607543395685,"deepnote_cell_type":"code"},"source":"### Multi Dimensional Example - simple case\nx = AD.AADVariable(1.0, [1, 0]) # x = 1.0\ny = AD.AADVariable(2.0, [0, 1]) # y = 2.0\nscalar = 6.0\n\nf = x + y - scalar\n\nprint(f)","execution_count":null,"outputs":[{"name":"stdout","text":"AADVariable fun = -3.0, der = [1 1]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Advancing the Multidimensional Case - using previously declared AAD variables\n\n**Function & Evaluation**\n\n$ f(x,y) = \\frac{x+ \\frac{y}{2}}{10} $ *at x = 1.0 and y = 2.0* \n\n$ f(1.0,2.0) = \\frac{1.0+ \\frac{2.0}{2}}{10} = 0.2$\n\n**Partial Derivative with respect to X**\n\n$ \\frac{\\partial f}{\\partial x} (x,y) = 1.0/10 $\n\n$ \\frac{\\partial f}{\\partial x} (1.0,2.0) = 0.1 $\n\n**Partial Derivative with respect to Y**\n\n$ \\frac{\\partial f}{\\partial y} (x,y) = 0.5/10 $\n\n$ \\frac{\\partial f}{\\partial y} (1.0,2.0) = 0.05 $\n\n**Derivative Evaluation**\n\n$ f'(1.0,2.0) = \\begin{bmatrix}\n0.1 & 0.05\n\\end{bmatrix}$ \n\n","metadata":{"tags":[],"cell_id":"00018-8e0d38bf-893e-4719-9429-93319727e6f7","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00018-d6a80ec5-0fed-43d1-9297-96ef3ac62307","output_cleared":false,"source_hash":"a5761958","execution_millis":2,"execution_start":1607543396724,"deepnote_cell_type":"code"},"source":"#Advancing Multi Dimensional using previously declared AAD Variables\n\nprint((x + 0.5*y)/10) # 0.1x+0.05y","execution_count":null,"outputs":[{"name":"stdout","text":"AADVariable fun = 0.2, der = [0.1  0.05]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Multi Dimensional Example - Adding a Variable","metadata":{"tags":[],"cell_id":"00010-a2559513-6f87-41f6-82cf-8fbc71521f2f","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"**Function & Evaluation**\n\n$ f(x,y,z) = x \\times y \\times z$ *at x = 2.0, y = 1.0 and z = 4.0*\n\n$ f(2.0,1.0,4.0) = 2.0 \\times 1.0 \\times 4.0 = 8.0$ \n\n**Partial Derivative with respect to X**\n\n$ \\frac{\\partial f}{\\partial x} (x,y,z) = y \\times z$\n\n$ \\frac{\\partial f}{\\partial x} (2.0,1.0,4.0)  = 1.0 \\times 4.0 = 4.0$\n\n**Partial Derivative with respect to Y**\n\n$ \\frac{\\partial f}{\\partial y} (x,y,z) = x \\times z $ \n\n$ \\frac{\\partial f}{\\partial y} (2.0,1.0,4.0)  = 2.0 \\times 4.0 = 4.0 = 8.0$ \n\n**Partial Derivative with respect to Z**\n\n$ \\frac{\\partial f}{\\partial z} (x,y,z) = x \\times y $\n\n$ \\frac{\\partial f}{\\partial z} (2.0,1.0,4.0) = 2.0 \\times 1.0 = 2.0$\n\n**Derivative Evaluation**\n\n$ f'(2.0,1.0,4.0) = \\begin{bmatrix}\n4.0 & 8.0 & 2.0\n\\end{bmatrix}$ \n\n","metadata":{"tags":[],"cell_id":"00021-c9e47190-51f1-4eb8-af65-628aa5da2d15","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"The AADVariable parser automatically zero-pads required derivative arrays, thus both of the following syntaxes work\n\n","metadata":{"tags":[],"cell_id":"00022-499a40f3-ee8d-48b9-85a2-c018b0c0e3a4","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00017-fe20be4f-8182-43be-b931-93678b8e11e9","output_cleared":false,"source_hash":"59503c7","execution_millis":3,"execution_start":1607543398361,"deepnote_cell_type":"code"},"source":"### Multi Dimensional Example - complex case\nx = AD.AADVariable(2.0, [1, 0, 0]) # x = 2.0\ny = AD.AADVariable(1.0, [0, 1, 0]) # y = 1.0\nz = AD.AADVariable(4.0, [0, 0, 1]) # z = 4.0\n\nf = x*y*z\nprint(x*y*z)\n\nx = AD.AADVariable(2.0, [1]) # x = 2.0\ny = AD.AADVariable(1.0, [0, 1]) # y = 1.0\nz = AD.AADVariable(4.0, [0, 0, 1]) # z = 4.0\nprint('We get the same result')\n\nf = x*y*z\nprint(x*y*z)","execution_count":null,"outputs":[{"name":"stdout","text":"AADVariable fun = 8.0, der = [4. 8. 2.]\nWe get the same result\nAADVariable fun = 8.0, der = [4. 8. 2.]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Vector Valued Functions","metadata":{"tags":[],"cell_id":"00011-4f82cc4c-e955-4646-8db4-9586a648de41","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"**Function & Evaluation**\n\n$f(x,y,z) = \\begin{bmatrix}\nx+y \\times z\\\\\nz \\times x-y \n\\end{bmatrix}$ *at x = 3.0, y = 2.0*, and z=1.0*\n\n$f(3.0,2.0,1.0) = \\begin{bmatrix}\n3.0+2.0 \\times 1.0\\\\\n1.0 \\times 3.0-2.0 \n\\end{bmatrix} = \n\\begin{bmatrix}\n5.0\\\\\n1.0 \n\\end{bmatrix}\n$\n\n**Partial Derivative with respect to X**\n\n$\\frac{\\partial f}{\\partial x} (x,y,z) = \\begin{bmatrix}\n1.0\\\\\nz \n\\end{bmatrix}$\n\n$\\frac{\\partial f}{\\partial x} (3.0,2.0,1.0) = \\begin{bmatrix}\n1.0\\\\\n1.0 \n\\end{bmatrix}$\n\n**Partial Derivative with respect to Y**\n\n$\\frac{\\partial f}{\\partial y} (x,y,z) = \\begin{bmatrix}\nz\\\\\n-1.0 \n\\end{bmatrix}$\n\n$\\frac{\\partial f}{\\partial y} (3.0,2.0,1.0) = \\begin{bmatrix}\n1.0\\\\\n-1.0 \n\\end{bmatrix}$\n\n**Partial Derivative with respect to Z**\n\n$\\frac{\\partial f}{\\partial Z} (x,y,z) = \\begin{bmatrix}\ny\\\\\nx \n\\end{bmatrix}$\n\n$\\frac{\\partial f}{\\partial Z} (3.0,2.0,1.0) = \\begin{bmatrix}\n2.0\\\\\n3.0 \n\\end{bmatrix}$\n\n**Derivative Evaluation**\n\n$ f'(2.0,1.0,4.0) = \\begin{bmatrix}\n1.0 & 1.0 & 2.0\\\\\n1.0 & -1.0 & 3.0\n\\end{bmatrix}$ ","metadata":{"tags":[],"cell_id":"00025-6ac84afb-8cd7-4c2f-8e84-6a2e68a8ac51","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00025-0e386e14-abe4-4da7-a742-a71c148abb89","output_cleared":false,"source_hash":"fbe6de39","execution_millis":1,"execution_start":1607543400640,"deepnote_cell_type":"code"},"source":"\nx = ADF.AADVariable(3.0, [1]) # either not specifying der, [1], or [1 0] or [1,0,0] will all work, as above\ny = ADF.AADVariable(2.0, [0, 1]) \nz = ADF.AADVariable(1.0, [0, 0, 1]) \n\nf_1 = x+(y*z)\nf_2 = (z*x)-y\nf = ADF.AADFunction([f_1, f_2])\nprint(f)","execution_count":null,"outputs":[{"name":"stdout","text":"[AADFunction fun = [5.0, 1.0], der = [[ 1.  1.  2.]\n [ 1. -1.  3.]]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Added Feature (Root Finder): Newton's Method, GMRES, & Gradient Descent","metadata":{"tags":[],"cell_id":"00019-f18d7bc5-770c-4658-a9d3-0a5162dd4402","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### Newton's Method\n- uses the `AAD`'s automatic differentiation core to solve systems of linear and non-linear equations. \n- solves or a system of $k$ (nonlinear)equations and continuously differentiable function $F: \\mathbb{R}^k \\rightarrow \\mathbb{R}^k$ \n- we have the Jacobian matrix $J_F(x_n)$ yielding\n\n$$\nx_{n+1} = x_n - J_F(x_n)^{-1} F(x_n)\n$$\n\n- Inverting the Jacobian is expensive and unstable\n\n- solving the following **linear** system is more practical:\n\n$$\nJ_F(x_n)(x_{n+1}-x_n) = -F(x_n)\n$$\n\nWe implement this using our automatic differentiation core, which retrieves the Jacobian $J_F(x_n)$ accurate to machine precision\n\n\nIn the future we can also expand Newton's Method solver to a \"least squares sense\" solution problem by using a generalized inverse of a non-square Jacobian matrix\n$J^\\dagger = (J^TJ)^{-1}J^T$ for non-linear functions, thus expanding our optimization toolbox.","metadata":{"tags":[],"cell_id":"00028-e211bc07-cd63-4f23-a52a-3f44a9510593","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### Gradient Descent\n- Uses the gradient to minimize the function and follow the descent of the funciton \ntopology. \n- Updates the location and obtaining the gradient at that location of the function namely:\n\n$$\na_{n+1}=a_n-\\gamma\\nabla F(a_n)\n$$\n\nWhere $a_{n+1}$ is the new point based on the previous point $a_n$ updated by the gradient of the function evaluated at the \nprevious point $$\\gamma\\nabla F(a_n)$$, where $$\\gamma$$ provides the step size. ","metadata":{"tags":[],"cell_id":"00029-70eac6a2-d534-4e97-bb75-a093f9c9cbb2","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### GMRES\n- uses `AAD`'s automatic differentiation core to solve systems of **linear** equations.\n\n- Given a system of $k$ linear equations for the function $F: \\mathbb{R}^k \\rightarrow \\mathbb{R}^k$ with a nonsingular Jacobian matrix\n$J_F(x_n)$\n- *Iteratively* solve by applying the GMRES solver to the **action** of the operator $F$ on a seed vector $v$, to yield $\\Delta x_k$ for the\nnext iteration.\n\n\n\n$$\na = J_F(x_n) v\n$$\n\nIs all that is needed for GMRES to solve for $J_F(x_k)\\Delta x_{k} = -F(x_k)$, and thus does not require the computation of the Jacobian itself.\n\nUsing this principle we can iteratively solve for $\\Delta x_k$ and improving the guess until convergence.","metadata":{"tags":[],"cell_id":"00030-6335bb56-85f7-4d55-a376-92d12f716855","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00028-b3f86304-4e0f-4205-9d74-c8c8bd08503d","output_cleared":false,"source_hash":"bbcbf1e4","execution_millis":2,"execution_start":1607544609138,"deepnote_cell_type":"code"},"source":"# Importing Dependencies\nfrom solvers.Newton import AAD_Newton\nfrom solvers.GradientDescent import AAD_grad\nfrom solvers.GMRES import AAD_GMRES\n\n#x0 = [x + 0.1 if x == 0 else x for x in x0] # Avoids singular value in the seed vector","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Finding the roots of the one-dimensional equation**\n\n$ f(x) = 3x-2$\n\n$ 0 = 3x-2$\n\n$ 0 = 3x-2$\n\n$ 2/3 = x$\n\n\n\n","metadata":{"tags":[],"cell_id":"00021-c1b047ac-f0d9-4b2b-a48c-29fab94ee1a6","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00021-574a542a-2fb2-47e6-bf03-a0bd10438e7b","output_cleared":false,"source_hash":"17be23af","execution_millis":5,"execution_start":1607543405473,"deepnote_cell_type":"code"},"source":"#Newton's Method\nx = AD.AADVariable(3, [1 ,0])\ninitial_guess = [30]\nsolve = AAD_Newton.solve(lambda x: [3*x-2], initial_guess)\nprint(solve)","execution_count":null,"outputs":[{"name":"stdout","text":"[0.6666666666666666]\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00031-80d0388d-bf69-4713-a0c0-0d6c8d9d3abb","output_cleared":false,"source_hash":"1c388824","execution_millis":7337,"execution_start":1607543406938,"deepnote_cell_type":"code"},"source":"#Gradient Descent\nx = AD.AADVariable(3, [1 ,0])\ngamma = .0001\nsolve = AAD_grad.solve(lambda x: AD.abs(3*x[0]-2), [x], gamma)\nprint(solve[0].val)","execution_count":null,"outputs":[{"name":"stdout","text":"0.6665999999995169\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00032-4447b182-d054-4dca-b513-2c031b0e0f1e","output_cleared":false,"source_hash":"5e7063db","execution_millis":2,"execution_start":1607543414279,"deepnote_cell_type":"code"},"source":"#GMRES\nx = AD.AADVariable(3, [1 ,0])\ninitial_guess = [30]\nsolve = AAD_GMRES.solve(lambda x: [3*x-2], initial_guess)\nprint(solve[0])","execution_count":null,"outputs":[{"name":"stdout","text":"0.6666666666666679\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Finding the roots of the multi-dimensional equation**\n\n$f(x,y) = \\begin{bmatrix}\n3 \\times x-y+1\\\\\nx\n\\end{bmatrix} $\n","metadata":{"tags":[],"cell_id":"00024-1cdd24c6-b303-43e9-9c94-34fdf965a837","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00037-e323e0ef-4ea2-43ec-8163-db7c031b76e2","output_cleared":false,"source_hash":"3c3a2807","execution_millis":3,"execution_start":1607544017865,"deepnote_cell_type":"code"},"source":"#Newton's Method\nx = AD.AADVariable(3, [1 ,0])\ny = AD.AADVariable(2, [0, 1])\ninitial_x = 0.5\ninitial_y = 1.5 \nsolve = AAD_Newton.solve(lambda x, y: [3*x-y+1,x], [initial_x, initial_y])\nprint(solve)","execution_count":null,"outputs":[{"name":"stdout","text":"[0.0, 1.0]\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00038-a123931b-ed49-4032-abe8-4a5c2d7d5172","output_cleared":false,"source_hash":"df920115","execution_millis":10864,"execution_start":1607544119336,"deepnote_cell_type":"code"},"source":"x = AD.AADVariable(3, [1 ,0])\ny = AD.AADVariable(2, [0, 1])\ngamma = .001\nsolve = AAD_grad.solve(lambda x: AD.abs(3*x[0]-x[1]+1),[x,y],gamma)\nprint(solve[1].der)","execution_count":null,"outputs":[{"name":"stdout","text":"[0 1]\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00039-e6342c35-fdb4-405a-8f53-7cd42f71af3b","output_cleared":false,"source_hash":"30a848dc","execution_millis":4,"execution_start":1607544627626,"deepnote_cell_type":"code"},"source":"#GMRES\nx = AD.AADVariable(3, [1 ,0])\ny = AD.AADVariable(2, [0, 1])\ninitial_guess = 0.5\ninitial_y = 1.5 \nsolve = AAD_GMRES.solve(lambda x, y: [3*x-y+1,x], [initial_x, initial_y])\nprint(solve)","execution_count":null,"outputs":[{"name":"stdout","text":"[0.0, 0.9999999999999998]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Concluding Sections","metadata":{"tags":[],"cell_id":"00041-b1ebb912-085c-44e2-953e-623b5479ecb5","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Broader Impact & Inclusivity\n\n- We have a diverse interdisciplinary team\n    - Lived experience and cultural background\n    - Academic background - environmental sciences, engineering, and public policy\n- Code was assessed by all group members and also inlcuded genuine and vulnerable group discssion\n- Our team would HIGHLY benefit from contribution from women - the first and most important addition we see moving forward\n- Inclusivity and impact are never fnished\n- We will keep flexible and open minds to continue thinking about how are software can be improved both in it's quantitative power and who it effects.\n","metadata":{"tags":[],"cell_id":"00041-517db7ab-1f31-46c9-a3a7-f8cae6652ff2","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Future Features & Ideas\n\n### Reverse Mode\n\n- implement the reverse mode scheme of automaotic differentiation. Having this toolset in addition to optimizers \nis extremely valuable in the field of AI as it allows one to efficiently compute the backwardpropagation\nof errors in multilayer perceptions (i.e. neural networks). \n- The implementation of backwardpropagation of neural networks is essnetiall a special case of reverse mode AD with an added set of \nelementary functions. These elementary functions are called activation functions some of which are already implemented in this version\n of the software. \n \n- Here is a comprehensive list  of new and already implemented functions we plan on incorporating into the future version of this software.\n\n**New Functions**: `relu`, `leaky relu`, `sigmoid`, `swish`, `softmax`, `binary step`\n\n**Already implemented**: `linear`, `tanh`\n\n### Optimizers\n\n- Add personalized optimizers and root finders\n- Users can define their own algorithm and compare to other models\n- Create a visualization tool set where users are able to compare the training, testing, and timing performce of their code\n- Compares to the built in optimizatio/root-finding algorithm.\n","metadata":{"tags":[],"cell_id":"00042-2e16c35a-2ec4-49ae-89ac-f29b9ba576a6","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## In Summation\n\n\n - **Background & Introduction**\n\n    - comprehensive and mathematically sound\n \n - **Implementation/Software Organization/How to Use**\n    - Intitutive for users in terms of structure and outward facing functions\n    - Offering a variety of features that can be accessed in multiple ways\n \n - **Additional Features/Extension**\n    - Groundwork for optimiziation\n    - Fully functioning tool set that has a variety of options for comparison\n\n - **Impact and Inclusivity**\n    - A diverse team with positive intentions\n    - Can always be improved\n \n - **Future Work & Other Possible Extensions**\n    - Enhanced methods of optimization that build on our codebase intuitively","metadata":{"tags":[],"cell_id":"00042-8489b9b4-e126-4d4b-a63b-46733bc8ad29","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### We appreciate your time and patience watching this presentation. We hope it has been informative. Please Email us with any questions - we will respond promptly.\n\nbenjaminmanning@hks.harvard.edu\n\nfernandes@g.harvard.edu\n\nhplin@g.harvard.edu","metadata":{"tags":[],"cell_id":"00045-4e41e006-0f95-4499-aab2-68d07fee9262","deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"","metadata":{"tags":[],"cell_id":"00046-0c6d2073-e656-4ce7-a522-429547857bfb","deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"327d9586-226d-4d94-85c8-17865c6335bf","deepnote_execution_queue":[]}}